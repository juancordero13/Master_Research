####################################################################################################
#######################-- BLOCK 0: SETTING UP THE ENVIRONMENT --####################################
####################################################################################################

install_if_missing <- function(package_name) {
  if (!require(package_name, character.only = TRUE)) {
    install.packages(package_name)
    library(package_name, character.only = TRUE)
  }
}

install_if_missing("tseries")
install_if_missing("quantmod")
install_if_missing("evmix")
install_if_missing("formattable")
install_if_missing("moments")
install_if_missing("forecast")
install_if_missing("fGarch")
install_if_missing("evir")
install_if_missing("openxlsx")
install_if_missing("purrr")
install_if_missing("ggplot2")
install_if_missing("ggthemes")
install_if_missing("eva")
install_if_missing("goftest")
install_if_missing("quarks")
install_if_missing("xts")

# Loading the necessary libraries
library(quantmod)
library(evmix)
library(formattable)
library(moments)
library(tseries)
library(forecast)
library(fGarch)
library(evir)
library(openxlsx)
library(purrr)
library(ggplot2)
library(ggthemes)
library(Metrics)
library(eva)
library(goftest)
library(quarks)
library(xts)

####################################################################################################
###########################-- BLOCK 1: DEFINING FUNCTIONS --########################################
####################################################################################################

# Defining a function for plotting the time series of the stocks and returns
plot_series <- function(list_of_stocks) {

    list_of_returns <- lapply(list_of_stocks, function(x) diff(log(x)) * 100)
    # Plotting both the stock prices and the returns over time
    lapply(list_of_stocks, function(x) print(plot(x, main = paste0("Stock of ", names(x), " over time"),
                                            xlab = "Time", ylab = "Yields")))
    lapply(list_of_returns, function(x) print(plot(x, main = paste0("Returns of ", names(x), " over time"),
                                        xlab = "Time", ylab = "Yields")))
    hist_plot <- function(series_df) {
        # Basic histogram
        ggplot(series_df, aes(x = series_df[, ncol(series_df)])) + 
        geom_histogram(aes(y = ..density..), colour = "black", fill = "white")+
        geom_density(alpha = .2, fill = "#FF6666") +
        ggtitle(paste0("Histogram of ", names(series_df))) +
        xlab("Returns") +
        ylab("Density")
        }

    lapply(list_of_returns, function(x) hist_plot(x))
    }

# Defining a function to obtain some descriptive statistics of the returns
desc_stats <- function(returns) {

    # Obtaining some descriptive statistics
    returns <- na.omit(returns)
    sample_size <- length(returns)
    returns_mean <- mean(returns)
    returns_median <- median(returns)
    returns_max <- max(returns)
    returns_min <- min(returns)
    returns_sd <- sd(returns)
    returns_kurtosis <- (1 / length(returns)) *
                        sum((returns - mean(returns))^4) / sd(returns)^4 - 3
    jb_test <- jarque.test(as.vector(returns))
    jb_p_value <- jb_test$p.value
    returns_skewness <- skewness(returns)
    adf_returns <- adf.test(returns)
    adf_p_value = adf_returns$p.value[1]
    st_list <- list(sample_size, returns_mean, returns_median, returns_max, returns_min,
                    returns_sd, returns_kurtosis, returns_skewness, jb_p_value,
                    adf_p_value)
    desc_stats <- data.frame(do.call(cbind, st_list))
    colnames(desc_stats) <- c("Sample size", "Mean", "Median", "Max", "Min", "Sd", "Kurtosis",
                             "Skewness", "Jarque-Bera", "ADF")
    rownames(desc_stats) <- c("returns")

    return(desc_stats)
    }

# Defining a function to standarize the series by subtracting the mean and dividing by
# subtracting the mean and dividing by the conditional standard deviation
conditional_standarization <- function(returns, aparch = FALSE) {

    # Multiply for (-1) because we are interested in the extreme losses
    returns <- returns * (-1)

    # Using an EWMA to estimate the conditional standard deviation
    if (aparch) {
        aparch_model <- garchFit(~aparch(1,1), data = returns, cond.dist = c("std"),
            trace = FALSE)

        mu <- coef(aparch_model)[1]
        volatility <- volatility(aparch_model)

        standarized_returns <- (returns - mu) / volatility
    } else {
        volatility <- ewma(returns)
        standarized_returns <- (returns - mean(returns)) / volatility
    }

    # Returning the standarized returns
    return(standarized_returns)
}

# Defining a function for calculating VaR and ES
# The function also calculate the GPD quantiles obtained for each optimal threshold value
market_risk_measure <- function(returns, conf_level, size_train, threshold_value) {

    # Calculating the VaR and the ES for the returns
    # Length of the returns vector
    T <- length(returns)

    # Splitting the dataset between training and testing sets
    train_set <- head(returns, round(length(returns) * size_train))
    h <- length(returns) - length(train_set)
    test_set <- tail(returns, h)

    # Getting the lengths of the training and testing sets
    outsample <- length(test_set)
    insample <- length(train_set)

    # Defining empty list where we'll store the results later
    VaR <- c()
    ES <- c()
    gpd_quantiles <- c()
    
    # Looping over the returns
    for(j in 0:(outsample - 1)) {

        print(paste0("Iteration ", j+1, " of ", outsample, " iterations"))

        # Training set that will be used in this iteration
        return_train <- returns[(1+j):(insample+j)]

        # Fitting an APARCH(1,1) model to the training set
        # Using as a conditional distribution the normal distribution
        aparch_fit <- garchFit(~aparch(1,1), data = return_train, cond.dist = c("std"),
            trace = FALSE)

        # Storing the parameters of the fitted model
        mu <- coef(aparch_fit)[1]
        omega <- coef(aparch_fit)[2]
        alpha <- coef(aparch_fit)[3]
        gamma <- coef(aparch_fit)[4]
        beta <- coef(aparch_fit)[5]
        delta <- coef(aparch_fit)[6]

        # Getting the residuals of the fitted model as value - conditional mean
        residuals <- return_train - mu
        # Residual value for the last observation
        residual_t <- residuals[insample]

        # Storing the conditional sd of the fitted model
        sigmas <- volatility(aparch_fit)
        # Conditional sd for the last observation
        sigma_t <- sigmas[insample]

        # Plotting the conditional volatility of the model
        # plot(sigmas, type = "l", main = "Conditional volatility of the APARCH(1,1) model",
        #     xlab = "Time", ylab = "Conditional volatility")

        # Forecasting the conditional volatility of the model one day ahead
        frcst_sigma <- (omega + 
            alpha * (abs(residual_t) - gamma * residual_t) ^ delta +
            beta * sigma_t ^ delta) ^ (1 / delta)

        # Standarizing the returns
        z <- (return_train - mu) / sigmas
        # Multiplying the returns by (-1)
        # We are interested in the extreme losses, not in the extreme gains
        z <- z * (-1)

        # Fitting a Generalized Pareto Distribution to the exceedances
        gpd_fit <- gpd(z, threshold_value)

        # Getting the GPD quantile
        gpd_quantile <- riskmeasures(gpd_fit, conf_level)[,2]
        # And the Expected Shortfall
        gpd_es <- riskmeasures(gpd_fit, conf_level)[,3]

        # Quantifying market risk: forecasting the VaR and ES
        frcst_var <- mu + frcst_sigma * gpd_quantile * (-1)
        frcst_es <- mu + frcst_sigma * gpd_es * (-1)

        # Storing the results of each iteration in a list
        gpd_quantiles <- c(gpd_quantiles, gpd_quantile)
        VaR <- c(VaR, frcst_var)
        ES <- c(ES, frcst_es)
        }

    # Storing the results in a dataframe
    mrm_df <- data.frame(cbind(VaR, ES))

    # Getting the main descriptive statistics of the results
    stats_gpd <- c(mean(gpd_quantiles), sd(gpd_quantiles), min(gpd_quantiles), max(gpd_quantiles))
    stats_var <- c(mean(mrm_df$VaR), sd(mrm_df$VaR), min(mrm_df$VaR), max(mrm_df$VaR))
    stats_es <- c(mean(mrm_df$ES), sd(mrm_df$ES), min(mrm_df$ES), max(mrm_df$ES))

    stats_mrm <- data.frame(cbind(stats_gpd, stats_var, stats_es))

    return(list(gpd_quantiles, mrm_df, stats_mrm))
    }

# Defining a function for application Li et al (2014) RMSE based method to select threshold
Li_method <- function(returns, min_threshold, range_length, max_threshold) {

    # Making a list of thresholds of length 100 from the initial threshold to the 99.9th percentile
    u_seq <- seq(quantile(returns, min_threshold), quantile(returns, max_threshold), 
        length.out = range_length)

    # Creating an empty list to store later the RMSE values
    rmse_values <- rep(NA, length(u_seq))

    # Iterating over the thresholds
    for (i in seq_along(u_seq)) {
        # cat("ITERATION NUMBER: ", i,"\n")

        # For each threshold, calculate exceedances,
        u <- u_seq[i]

        # fitting the gpd to these exceedances
        # print("Fitting the GPD... \n")
        # cat("Number of exceedances: ", length(exceedances),"\n")
        # cat("Threshold: ", u,"\n")
        gpd_instance <- evir::gpd(data = returns, threshold = u)

        exceedances <- returns[returns > u] - u
        # calculate the fitted values
        # print("Getting the fitted values...\n")
        fitted_values <- evir::pgpd(exceedances,
            xi = gpd_instance$par.ests["xi"],
            beta = gpd_instance$par.ests["beta"])

        # print(qqplot(exceedances, fitted_values))
        # abline(0,1)

        # calculate the Root Mean Squared Error (RMSE)
        # print("Calculating the RMSE...\n")
        rmse_values[i] <- rmse(ecdf(exceedances)(exceedances), fitted_values)

        print("--------------------------------------------------------------------------------")
        cat("Threshold value: ", u_seq[i],"\n")
        cat("RMSE: ", rmse_values[i],"\n")
        cat("Number of exceedances: ", length(exceedances),"\n")
        # cat(plot(fitted_values, ecdf(exceedances)(exceedances)), "\n")
    }

    # Retrieving the thresholds that minimizes the RMSE
    best_u <- u_seq[which.min(rmse_values)]

    print(ggplot(data.frame(u_seq, rmse_values), aes(x = u_seq, y = rmse_values)) +
    geom_line() +
    geom_vline(xintercept = best_u, linetype = "dashed", color = "red") +
    xlab("Threshold") +
    ylab("RMSE"))


    return(best_u)
    }

# Defining a function for application of V. Choulakian and M. A. Stephens (2001) method based
# on the Cramer Von Mises and Anderson Darling goodness of fit tests
Choukalian_method <- function(returns, initial_threshold) {

    p_values <- c()
    p_value <- 0
    u <- quantile(returns, initial_threshold)
    list_u <- c()
    while (TRUE) {

        # Calculate the returns above the threshold
        extreme_returns <- returns[returns > u]
        # For each threshold, calculate exceedances
        exceedances <- returns[returns > u] - u

        # Applying the Cramer Von Mises test for the exceedances assuming they follow a GPD
        cvm_test <- gpdCvm(exceedances)

        p_value <- cvm_test$p.value

        cat("Threshold value: ", u,"\n")
        cat("p-value: ", p_value,"\n")
        cat("Number of exceedances: ", length(exceedances),"\n")
        cat("--------------------------------------------------------------------------------\n")

        # Getting the p-values for each threshold
        p_values <- c(p_values, p_value)
        list_u <- c(list_u, u)

        if (p_value > 0.1) {
            print("We found the optimal threshold before the loop was ended.")
            break
        } else {
            u <- u + (min(extreme_returns) - u)
        }
    
    }

    # Plotting the p-values against the range of threholds
    print(ggplot(data.frame(list_u, p_values), aes(x = list_u, y = p_values)) +
        geom_line() + xlab("Thresholds") + ylab("P-values"))

    return(u)
    }

####################################################################################################
################-- BLOCK 2: RETRIEVING, PROCCESSING AND PLOTTING THE DATA --########################
####################################################################################################

# Getting the series of stocks vía Yahoo Finance API
for (code in c("BTC-USD", "GBPUSD=X", "GC=F", "BZ=F", "^GSPC")) {
    getSymbols(code, from = "2000-01-01", to = "2023-02-28")
}
sp500 <- `GSPC`$`GSPC.Adjusted`
btc_usd <- `BTC-USD`$`BTC-USD.Adjusted`
gbp_usd <- `GBPUSD=X`$`GBPUSD=X.Adjusted`
gold_usd <- `GC=F`$`GC=F.Adjusted`
brent_usd <- `BZ=F`$`BZ=F.Adjusted`

# Getting flow data of the spanish river Ebro from 2000-10-01 to 2019-09-30
# obtained from CEDEX (Centro de Estudios y Experimentación de Obras Públicas)
ebro_flow <- read.csv("/Users/juancordero/Desktop/My_GitHub/Master_Research/Input/caudal_ebro.csv",
    header = TRUE, sep = ",")
ebro_flow <- ebro_flow[, c(2, 4)]

# Converting the dataframe to xts object for further processing
ebro_flow$date <- as.Date(ebro_flow$date, format = "%d/%m/%Y")
ebro_flow <- xts(ebro_flow$flow, order.by = ebro_flow$date)

# Building a list with all the series of stocks to iterate over
all_stocks <- list(sp500, btc_usd, gbp_usd, gold_usd, brent_usd, ebro_flow)

# Also building a list with all the returns series (log differences)
all_returns <- lapply(all_stocks, function(x) na.omit(diff(log(x)) * 100))

# Also, since we are interested in the extreme losses, we multiply the returns by (-1)
inv_returns <- lapply(all_returns, function(x) x * (-1))

# We standarize the returns and multiply by (-1) since we are interested in the extreme losses.
# Here, we should standarize the returns with conditional sd instead of unconditional sd.
transformed_returns <- lapply(all_returns, function(x) conditional_standarization(x,
    aparch = TRUE))

# Calculate main descriptive statistics for the returns series
descriptive_stats <- data.frame()
for (returns in all_returns) {
    descriptive_stats <- rbind(descriptive_stats, desc_stats(returns))
}
rownames(descriptive_stats) <- c("S&P500", "BTC/USD", "GBP/USD", "GOLD/USD",
    "Crude Oil Brent", "Ebro River Flow")
print(formattable(descriptive_stats))

# Plotting stocks over time, returns over time and histogram of returns for all series
plot_series(list_of_stocks = all_stocks)

####################################################################################################
#########-- BLOCK 3: CALCULATING OPTIMAL THRESHOLDS ACCORDING TO METHODOLOGIES --###################
####################################################################################################

# Upper 10% rule of DuMounchel
u_dumounchel <- lapply(transformed_returns, function(x) quantile(x, 0.90))

# Rule of Loretan and Philips (1994) for calculating the optimal threshold
u_loretan <- lapply(transformed_returns, function(x) quantile(x,
    1 - (length(x)**(2/3) / log(log(length(x))) / length(x))))

# Ferreira et al. (2003) rule: Extreme values as those above sqrt(n)
u_ferreira <- lapply(transformed_returns, function(x) quantile(x, 1 - sqrt(length(x)) / length(x)))

# Mean Excess Plot or Mean Residual Life Plot
par(mfrow = c(1, 1))
lapply(transformed_returns, function(x) print(mrlplot(as.vector(x),
    tlim = c(quantile(x, 0.8), quantile(x, 0.99)),
    nt = 20)))

# Storing the optimal values of the Mean Residual Life Plot manually
u_mrlplot <- c(1.3, 0.85, 1.2, 1.2, 1.2, NA)

# Parameter Stability Plot
par(mfrow = c(2, 1))
lapply(transformed_returns, function(x) print(tcplot(as.vector(x),
    tlim = c(quantile(x, 0.8), quantile(x, 0.99)),
    nt = 20)))
# Storing the optimal values of the Parameter Stability Plot manually
u_psplot <- c(1.3, 0.85, 1.2, 1.2, 1.2, NA)

# Hill Plot
par(mfrow = c(1, 1))
lapply(transformed_returns, function(x) print(hillplot(as.vector(x),
    tlim = c(quantile(x, 0.8), quantile(x, 0.99)))))
# Storing the optimal values of the Hill Plot manually
u_hillplot <- c(1.8, 1.3, 1.6, 1.6, 1.7, NA)

# Calling the function Li_rmse above to calculate optimal thresholds for each stock
u_li_rmse <- lapply(transformed_returns, function (x) Li_method(as.vector(x),
    min_threshold = 0.8, max_threshold = 0.995, range_length = 100))

# Defining manually river data to test Choukalian's method
# river_data <- c(1.7, 2.2, 14.4, 1.1, 0.4, 20.6, 5.3, 0.7, 1.9, 13, 12, 9.3, 1.4,
# 18.7, 8.5, 25.5, 11.6, 14.1, 22.1, 1.1, 2.5, 14.4, 1.7, 37.6, 0.6, 2.2, 39, 0.3, 15,
# 11, 7.3, 22.9, 1.7, 0.1, 1.1, 0.6, 9, 1.7, 7, 20.1, 0.4, 2.8, 14.1, 9.9, 10.4, 10.7, 30,
# 3.6, 5.6, 30.8, 13.3, 4.2, 25.5, 3.4, 11.9, 21.5, 27.6, 36.4, 2.7, 64, 1.5, 2.5,
# 27.4, 1, 27.1, 20.2, 16.8, 5.3, 9.7, 27.5, 2.5, 27)

# Calling the function Choukalian_method above to calculate optimal thresholds for each stock
u_choukalian <- lapply(transformed_returns, function (x) Choukalian_method(as.vector(x),
    initial_threshold = 0.8))

# Storing all the optimal thresholds in a dataframe (rows = stocks, columns = methodologies)
opt_thresholds <- data.frame(cbind(u_ferreira, u_loretan, u_dumounchel,
    u_li_rmse, u_mrlplot, u_psplot, u_hillplot, u_choukalian))
rownames(opt_thresholds) <- c("S&P500", "BTC/USD", "GBP/USD", "GOLD/USD",
    "Crude Oil Brent", "Ebro River Flow")
print(formattable(opt_thresholds))

####################################################################################################
############-- BLOCK 4: ESTIMATING MARKET RISK MEASURES FOR EACH THRESHOLD --#######################
####################################################################################################

# Getting the mean, sd, max and min of the VaR and ES for each stock
# Using the Upper 10% rule of DuMounchel
start_time <- Sys.time()

# Calculating VaR and ES for each stock and methodology

# Creating an excel file (workbook) where we will store the results
# There will be a sheet for each methodology, where rows = stats and columns = stocks
xlsx_workbook <- createWorkbook()

# Looping through the optimal thresholds (one for each methodology)
# and through the returns series and estimating VaR and ES
for (j in 1:ncol(opt_thresholds)) {

    cat("Applying methodology ",j,"of ",ncol(opt_thresholds),". \n")

    mrm_data <- list()
    mrm_stats <- list()
    pareto_quantiles <- list()
    for (i in 1:length(all_returns)) {
        mrm <- market_risk_measure(returns = all_returns[[i]],
            threshold_value = as.numeric(opt_thresholds[i, j]), conf_level = 0.99,
            size_train = 0.75)
        
        pareto_quantiles <- c(pareto_quantiles, mrm[[1]])
        mrm_data <- c(mrm_data, mrm[[2]])
        mrm_stats <- c(mrm_stats, mrm[[3]])
    }

    mrm_stats <- data.frame(mrm_stats)
    colnames(mrm_stats) <- c("Quantiles S&P500", "VaR S&P500", "ES S&P500", "Quantiles BTC/USD", "VaR BTC/USD", "ES BTC/USD",
        "Quantiles GBP/USD", "VaR GBP/USD", "ES GBP/USD", "Quantiles GOLD/USD", "VaR GOLD/USD", "ES GOLD/USD",
        "Quantiles Crude Oil Brent", "VaR Crude Oil Brent", "ES Crude Oil Brent", "Quantiles Ebro River Flow",
        "VaR Ebro River Flow", "ES Ebro River Flow")
    rownames(mrm_stats) <- c("Mean", "SD", "Min", "Max")

    addWorksheet(xlsx_workbook, sheetName = colnames(opt_thresholds)[j])
    writeData(xlsx_workbook, sheet = colnames(opt_thresholds)[j], x = mrm_stats,
              row.names = TRUE)

    saveWorkbook(xlsx_workbook,
                 file = "/Users/juancordero/Desktop/My_GitHub/Master_Research/Output/results.xlsx",
                 overwrite = TRUE)
}

end_time <- Sys.time()
cat("It took ", (end_time - start_time) / 60, " minutes to run the code. \n")

####################################################################################################
###########################-- BLOCK 5: TESTING SOME THINGS --#######################################
####################################################################################################

# Test to select inicial threshold value for iterations
test <- transformed_returns[[4]]
u_seq <- seq(0, 0.999, length.out = 1000)

for (perc in u_seq) {

    u <- quantile(test, perc)
    exceed <- test[test > u] - u
    exceed_per_year <- apply.yearly(exceed, FUN = length)

    exceed_per_year <- as.vector(exceed_per_year)
    if (length(exceed_per_year) != 24) {
        exceed_per_year <- c(exceed_per_year, rep(0, 24 - length(exceed_per_year)))
    }

    # print(exceed_per_year)

    mean_ex <- mean(exceed_per_year)
    var_ex <- var(exceed_per_year)

    cat("Threshold: ", u, "\n")
    # cat("Number of years with exceedances: ", length(exceed_per_year), "\n")
    cat("Mean Exceedances per Year: ", mean_ex, "\n")
    cat("Variance of Exceedances per Year: ", var_ex, "\n")
    cat("Ratio Mean/Variance: ", mean_ex / var_ex, "\n")
    cat("Number of total exceedances: ", sum(exceed_per_year), "\n")
    cat("------------------------------------------------------------- \n")

    if (mean_ex / var_ex > 1) {
        cat("The threshold is optimal. \n")
        break
    } else {
        cat("The threshold is not optimal. \n")
    }
}